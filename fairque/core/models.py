"""Core models for FairQueue: Priority, Task, and DLQEntry."""

import dataclasses
import json
import logging
import os
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum, IntEnum
from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Set, Tuple, Union

from fairque.core.exceptions import TaskSerializationError

if TYPE_CHECKING:
    from fairque.core.pipeline import Executable, ParallelGroup, Pipeline
    from fairque.core.xcom import XComManager

logger = logging.getLogger(__name__)


class TaskState(str, Enum):
    """Task execution states with unified state management."""

    QUEUED = "queued"        # Ready for execution
    DEFERRED = "deferred"    # Waiting for dependencies (managed in fq:state:queued)
    STARTED = "started"      # Currently executing
    SCHEDULED = "scheduled"  # Waiting for execute_after time
    FINISHED = "finished"    # Successfully completed
    FAILED = "failed"        # Execution failed (unified with DLQ functionality)
    CANCELED = "canceled"    # Manually canceled


class Priority(IntEnum):
    """Task priority levels with type safety."""

    VERY_LOW = 1    # Lowest priority, minimal time weight
    LOW = 2         # Low priority
    NORMAL = 3      # Standard priority
    HIGH = 4        # High priority
    VERY_HIGH = 5   # Very high priority, maximum time weight
    CRITICAL = 6    # Critical priority, uses separate FIFO queue

    @property
    def weight(self) -> float:
        """Get priority weight for score calculation (1-5 only).

        Returns:
            Priority weight as float between 0.2 and 1.0

        Raises:
            ValueError: If called on CRITICAL priority
        """
        if self == Priority.CRITICAL:
            raise ValueError("Critical priority does not use weight calculation")
        return self.value / 5.0

    @property
    def is_critical(self) -> bool:
        """Check if priority is critical.

        Returns:
            True if priority is CRITICAL, False otherwise
        """
        return self == Priority.CRITICAL

    @classmethod
    def from_int(cls, value: int) -> "Priority":
        """Create Priority from integer with validation.

        Args:
            value: Integer value (1-6)

        Returns:
            Priority enum instance

        Raises:
            ValueError: If value is not in valid range
        """
        try:
            return cls(value)
        except ValueError:
            raise ValueError(f"Invalid priority value: {value}. Must be 1-6.") from None


def calculate_score(task: "Task") -> float:
    """Calculate priority score for normal queue ordering.

    Higher score = higher priority for processing.
    Formula: created_at + (priority_weight * elapsed_time)
    This prevents starvation while respecting priority levels.

    Args:
        task: Task to calculate score for

    Returns:
        Priority score as float

    Raises:
        ValueError: If task has CRITICAL priority
    """
    if task.priority.is_critical:
        raise ValueError("Critical tasks do not use score calculation")

    current_time = time.time()
    elapsed_time = current_time - task.created_at
    priority_weight = task.priority.weight

    return task.created_at + (priority_weight * elapsed_time)


def detect_dependency_cycle(task_dependencies: Dict[str, List[str]], new_task_id: str, dependencies: List[str]) -> bool:
    """Detect if adding dependencies would create a cycle.

    Args:
        task_dependencies: Dict mapping task_id to its dependencies
        new_task_id: ID of the new task being added
        dependencies: List of task IDs the new task depends on

    Returns:
        True if adding dependencies would create a cycle, False otherwise
    """
    # Create temporary graph including the new task
    temp_graph = task_dependencies.copy()
    temp_graph[new_task_id] = dependencies.copy()

    def has_cycle_dfs(node: str, visited: Set[str], rec_stack: Set[str]) -> bool:
        """DFS to detect cycles in directed graph."""
        visited.add(node)
        rec_stack.add(node)

        # Check all dependencies of current node
        for dep in temp_graph.get(node, []):
            if dep not in visited:
                if has_cycle_dfs(dep, visited, rec_stack):
                    return True
            elif dep in rec_stack:
                return True

        rec_stack.remove(node)
        return False

    visited: Set[str] = set()

    # Check for cycles starting from any unvisited node
    for task_id in temp_graph:
        if task_id not in visited:
            if has_cycle_dfs(task_id, visited, set()):
                return True

    return False


@dataclass
class Task:
    """Optimized Task model with efficient serialization and function execution support."""

    task_id: str           # UUID4 auto-generated by system
    user_id: str
    priority: Priority     # Priority enum (1-6)
    payload: Dict[str, Any]
    retry_count: int = 0
    max_retries: int = 3
    created_at: float = field(default_factory=time.time)
    execute_after: float = field(default_factory=time.time)

    # Function execution support
    func: Optional[Callable[..., Any]] = field(default=None, compare=False, repr=False)
    args: Tuple[Any, ...] = field(default_factory=tuple, compare=False, repr=False)
    kwargs: Dict[str, Any] = field(default_factory=dict, compare=False, repr=False)

    # XCom configuration
    enable_xcom: bool = False
    xcom_push_key: Optional[str] = None
    xcom_pull_keys: Dict[str, str] = field(default_factory=dict)
    xcom_ttl_seconds: int = 3600
    xcom_namespace: str = "default"  # Default namespace

    # Dependency management
    depends_on: List[str] = field(default_factory=list)  # Task IDs this task depends on
    dependents: List[str] = field(default_factory=list)  # Task IDs that depend on this task
    state: TaskState = TaskState.QUEUED
    started_at: Optional[float] = None
    finished_at: Optional[float] = None
    result: Optional[Any] = field(default=None, repr=False, compare=False)
    auto_xcom: bool = False  # Automatically pass result to dependents via XCom

    # Enhanced failure tracking (unified DLQ functionality)
    failure_type: Optional[str] = None      # "failed"|"expired"|"poisoned"|"timeout"
    error_message: Optional[str] = None     # Detailed error description
    retry_history: List[Dict[str, Any]] = field(default_factory=list)  # Retry attempt history

    # XCom manager reference (injected by TaskHandler)
    _xcom_manager: Optional['XComManager'] = field(default=None, init=False, repr=False, compare=False)

    @classmethod
    def create(
        cls,
        user_id: Optional[str] = None,
        task_id: Optional[str] = None,
        priority: Priority = Priority.NORMAL,
        payload: Dict[str, Any] = {},   # noqa: B006
        max_retries: int = 3,
        execute_after: Optional[float] = None,
        func: Optional[Callable[..., Any]] = None,
        args: Tuple[Any, ...] = (),
        kwargs: Dict[str, Any] = {},  # noqa: B006
        auto_xcom_decorators: bool = True,
        # XCom parameters
        enable_xcom: bool = False,
        xcom_namespace: str = "default",
        xcom_ttl_seconds: int = 3600,
        # Dependency parameters
        depends_on: Optional[List[str]] = None,
        auto_xcom: bool = False,
    ) -> "Task":
        """Create new task with optional custom ID, XCom and dependency support.

        Args:
            user_id: User identifier (default: from environment USER)
            task_id: Custom task ID (default: auto-generated UUID)
            priority: Task priority
            payload: Task data payload
            max_retries: Maximum retry attempts
            execute_after: Timestamp when task should be executed (defaults to now)
            func: Optional function to execute
            args: Function arguments
            kwargs: Function keyword arguments
            auto_xcom_decorators: Automatically configure XCom from function decorators
            enable_xcom: Enable XCom functionality
            xcom_namespace: XCom namespace for data storage
            xcom_ttl_seconds: Default TTL for XCom data
            depends_on: List of task IDs this task depends on
            auto_xcom: Automatically pass result to dependents via XCom

        Returns:
            New Task instance
        """
        if user_id is None:
            user_id = os.environ.get("USER", "unknown")
            assert user_id, "User ID must be provided or set in environment"

        current_time = time.time()

        # Determine initial state based on execute_after and dependencies
        if execute_after and execute_after > current_time:
            initial_state = TaskState.SCHEDULED
        elif depends_on:
            initial_state = TaskState.DEFERRED
        else:
            initial_state = TaskState.QUEUED

        # Generate task_id if not provided
        effective_task_id = task_id if task_id is not None else str(uuid.uuid4())

        task = cls(
            task_id=effective_task_id,
            user_id=user_id,
            priority=priority,
            payload=payload,
            retry_count=0,
            max_retries=max_retries,
            created_at=current_time,
            execute_after=execute_after or current_time,
            func=func,
            args=args,
            kwargs=kwargs,
            enable_xcom=enable_xcom,
            xcom_namespace=xcom_namespace,
            xcom_ttl_seconds=xcom_ttl_seconds,
            depends_on=depends_on or [],
            state=initial_state,
            auto_xcom=auto_xcom,
        )

        # Auto-configure XCom from function decorators
        if auto_xcom_decorators and func:
            task._configure_xcom_from_decorators()

        return task

    def is_ready_to_execute(self) -> bool:
        """Check if task is ready to execute based on execute_after timestamp.

        Returns:
            True if task is ready to execute, False otherwise
        """
        return time.time() >= self.execute_after

    def is_ready_for_execution(self) -> bool:
        """Check if task is ready for execution (time and dependencies).

        Returns:
            True if task can be executed now, False otherwise
        """
        return (
            self.state == TaskState.QUEUED and
            self.is_ready_to_execute() and
            not self.depends_on  # No unresolved dependencies
        )

    def mark_started(self) -> "Task":
        """Mark task as started and return new instance.

        Returns:
            New Task instance with STARTED state and started_at timestamp
        """
        return dataclasses.replace(
            self,
            state=TaskState.STARTED,
            started_at=time.time()
        )

    def mark_finished(self, result: Optional[Any] = None) -> "Task":
        """Mark task as finished and return new instance.

        Args:
            result: Optional task execution result

        Returns:
            New Task instance with FINISHED state and finished_at timestamp
        """
        return dataclasses.replace(
            self,
            state=TaskState.FINISHED,
            finished_at=time.time(),
            result=result
        )

    def mark_failed(self, error: Optional[str] = None, failure_type: str = "failed") -> "Task":
        """Mark task as failed with enhanced failure tracking.

        Args:
            error: Optional error message
            failure_type: Type of failure (failed, expired, poisoned, timeout)

        Returns:
            New Task instance with FAILED state and failure metadata
        """
        current_time = time.time()

        # Add to retry history
        retry_entry = {
            "attempt": self.retry_count + 1,
            "failed_at": current_time,
            "error": error or "Unknown error",
            "failure_type": failure_type
        }

        new_retry_history = self.retry_history + [retry_entry]

        return dataclasses.replace(
            self,
            state=TaskState.FAILED,
            failure_type=failure_type,
            error_message=error,
            retry_history=new_retry_history,
            finished_at=current_time
        )

    def mark_deferred(self) -> "Task":
        """Mark task as deferred (waiting for dependencies).

        Note: Deferred tasks are managed within fq:state:queued but with
        state field set to 'deferred' for distinction.

        Returns:
            New Task instance with DEFERRED state
        """
        return dataclasses.replace(self, state=TaskState.DEFERRED)

    def mark_canceled(self) -> "Task":
        """Mark task as canceled.

        Returns:
            New Task instance with CANCELED state
        """
        return dataclasses.replace(
            self,
            state=TaskState.CANCELED,
            finished_at=time.time()
        )

    def add_dependent(self, task_id: str) -> "Task":
        """Add a dependent task ID.

        Args:
            task_id: ID of task that depends on this task

        Returns:
            New Task instance with updated dependents list
        """
        if task_id not in self.dependents:
            new_dependents = self.dependents + [task_id]
            return dataclasses.replace(self, dependents=new_dependents)
        return self

    def remove_dependency(self, task_id: str) -> "Task":
        """Remove a dependency (when dependency completes).

        Args:
            task_id: ID of completed dependency task

        Returns:
            New Task instance with updated depends_on list
        """
        if task_id in self.depends_on:
            new_depends_on = [dep for dep in self.depends_on if dep != task_id]
            # If no more dependencies and in deferred state, move to queued
            new_state = TaskState.QUEUED if not new_depends_on and self.state == TaskState.DEFERRED else self.state
            return dataclasses.replace(
                self,
                depends_on=new_depends_on,
                state=new_state
            )
        return self

    def __rshift__(self, other: "Union[Task, Executable]") -> "Pipeline":
        """Implement >> operator for task dependencies (self >> other).

        Args:
            other: Task or Executable that should depend on this task

        Returns:
            Pipeline representing the dependency relationship
        """
        from fairque.core.pipeline import Pipeline, TaskWrapper

        if isinstance(other, Task):
            return Pipeline([TaskWrapper(self), TaskWrapper(other)])
        else:
            return Pipeline([TaskWrapper(self), other])

    def __lshift__(self, other: "Union[Task, Executable]") -> "Pipeline":
        """Implement << operator for reverse dependencies (self << other).

        Args:
            other: Task or Executable that this task should depend on

        Returns:
            Pipeline representing the dependency relationship
        """
        from fairque.core.pipeline import Pipeline, TaskWrapper

        if isinstance(other, Task):
            return Pipeline([TaskWrapper(other), TaskWrapper(self)])
        else:
            return Pipeline([other, TaskWrapper(self)])

    def __or__(self, other: "Union[Task, Executable]") -> "ParallelGroup":
        """Implement | operator for parallel execution (self | other).

        Args:
            other: Task or Executable to execute in parallel with this task

        Returns:
            ParallelGroup containing both tasks
        """
        from fairque.core.pipeline import ParallelGroup, TaskWrapper

        if isinstance(other, Task):
            return ParallelGroup([TaskWrapper(self), TaskWrapper(other)])
        else:
            return ParallelGroup([TaskWrapper(self), other])

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        """Make Task callable. Execute the stored function or update arguments.

        Args:
            *args: Positional arguments for function execution or argument update
            **kwargs: Keyword arguments for function execution or argument update

        Returns:
            Function result if no arguments provided, or new Task instance with updated arguments

        Raises:
            ValueError: If no function is set and no arguments provided
        """
        if args or kwargs:
            # Update arguments and return new Task instance
            new_args = args if args else self.args
            new_kwargs = {**self.kwargs, **kwargs} if kwargs else self.kwargs
            return dataclasses.replace(self, args=new_args, kwargs=new_kwargs)
        else:
            # Execute the function
            return self.call_function()

    def call_function(self) -> Any:
        """Execute the stored function with arguments.

        Returns:
            Function execution result

        Raises:
            ValueError: If no function is set for this task
        """
        if self.func is None:
            raise ValueError("No function set for this task")
        return self.func(*self.args, **self.kwargs)

    def can_retry(self) -> bool:
        """Check if task can be retried.

        Returns:
            True if retry count is below max_retries, False otherwise
        """
        return self.retry_count < self.max_retries

    def get_retry_delay(self) -> float:
        """Calculate exponential backoff delay for retry.

        Returns:
            Delay in seconds for next retry attempt
        """
        return 2.0 ** self.retry_count

    def increment_retry(self) -> "Task":
        """Return new task instance with incremented retry count and updated execute_after.

        Returns:
            New Task instance with incremented retry count
        """
        delay = self.get_retry_delay()
        return dataclasses.replace(
            self,
            retry_count=self.retry_count + 1,
            execute_after=time.time() + delay,
        )

    # Serialization methods
    def to_json(self) -> str:
        """Convert task to JSON string.

        Returns:
            JSON string representation of the task

        Raises:
            TaskSerializationError: If serialization fails
        """
        try:
            task_dict = dataclasses.asdict(self)
            # Remove non-serializable fields
            task_dict.pop('func', None)
            task_dict.pop('_xcom_manager', None)
            # Convert enums to their values
            if 'priority' in task_dict:
                task_dict['priority'] = task_dict['priority'].value
            if 'state' in task_dict:
                task_dict['state'] = task_dict['state'].value
            return json.dumps(task_dict)
        except (TypeError, ValueError) as e:
            raise TaskSerializationError(f"Failed to serialize task to JSON: {e}") from e

    @classmethod
    def from_json(cls, json_str: str) -> "Task":
        """Create task from JSON string.

        Args:
            json_str: JSON string representation of the task

        Returns:
            Task instance

        Raises:
            TaskSerializationError: If deserialization fails
        """
        try:
            task_dict = json.loads(json_str)
            # Convert priority and state back to enums
            if 'priority' in task_dict:
                task_dict['priority'] = Priority(task_dict['priority'])
            if 'state' in task_dict:
                task_dict['state'] = TaskState(task_dict['state'])
            return cls(**task_dict)
        except (TypeError, ValueError, json.JSONDecodeError) as e:
            raise TaskSerializationError(f"Failed to deserialize task from JSON: {e}") from e

    # XCom methods
    def set_xcom_manager(self, xcom_manager: 'XComManager') -> None:
        """Set XCom manager reference (called by TaskHandler)."""
        self._xcom_manager = xcom_manager

    def xcom_push(self, key: str, value: Any, ttl_seconds: Optional[int] = None, namespace: Optional[str] = None) -> None:
        """Push value to XCom with namespace.

        Args:
            key: XCom key name
            value: Value to store
            ttl_seconds: TTL override (if None, use task's xcom_ttl_seconds)
            namespace: Namespace override (if None, use task's xcom_namespace)

        Raises:
            RuntimeError: If XCom is not enabled for this task
        """
        if not self.enable_xcom:
            raise RuntimeError("XCom is not enabled for this task. Set enable_xcom=True.")

        if not self._xcom_manager:
            raise RuntimeError("XCom manager not available. Task must be processed by XCom-enabled TaskHandler.")

        effective_ttl = ttl_seconds if ttl_seconds is not None else self.xcom_ttl_seconds
        effective_namespace = namespace if namespace is not None else self.xcom_namespace

        self._xcom_manager.push(
            key=key,
            value=value,
            user_id=self.user_id,
            namespace=effective_namespace,
            ttl_seconds=effective_ttl
        )

        ttl_info = "no TTL" if effective_ttl == 0 else f"TTL {effective_ttl}s"
        logger.debug(f"XCom pushed: key={key}, namespace={effective_namespace}, {ttl_info}")

    def xcom_pull(self, key: str, default: Any = None, namespace: Optional[str] = None) -> Any:
        """Pull value from XCom by key and namespace.

        Args:
            key: XCom key name
            default: Default value if key not found
            namespace: Specific namespace (if None, use task's namespace)

        Returns:
            Value from XCom or default

        Raises:
            RuntimeError: If XCom is not enabled for this task
        """
        if not self.enable_xcom:
            raise RuntimeError("XCom is not enabled for this task. Set enable_xcom=True.")

        if not self._xcom_manager:
            raise RuntimeError("XCom manager not available. Task must be processed by XCom-enabled TaskHandler.")

        effective_namespace = namespace if namespace is not None else self.xcom_namespace

        try:
            return self._xcom_manager.pull(key=key, user_id=self.user_id, namespace=effective_namespace)
        except KeyError:
            if default is not None:
                return default
            raise

    def xcom_pull_from_namespace(self, namespace: Optional[str] = None) -> Dict[str, Any]:
        """Pull all keys from a namespace.

        Args:
            namespace: Target namespace (if None, use task's namespace)

        Returns:
            Dictionary mapping keys to values

        Raises:
            RuntimeError: If XCom is not enabled for this task
        """
        if not self.enable_xcom:
            raise RuntimeError("XCom is not enabled for this task. Set enable_xcom=True.")

        if not self._xcom_manager:
            raise RuntimeError("XCom manager not available. Task must be processed by XCom-enabled TaskHandler.")

        effective_namespace = namespace if namespace is not None else self.xcom_namespace
        return self._xcom_manager.pull_from_namespace(effective_namespace, self.user_id)

    def xcom_clear_namespace(self, namespace: Optional[str] = None) -> int:
        """Clear all XCom data in a namespace.

        Args:
            namespace: Target namespace (if None, use task's namespace)

        Returns:
            Number of entries cleared

        Raises:
            RuntimeError: If XCom is not enabled for this task
        """
        if not self.enable_xcom:
            raise RuntimeError("XCom is not enabled for this task. Set enable_xcom=True.")

        if not self._xcom_manager:
            raise RuntimeError("XCom manager not available. Task must be processed by XCom-enabled TaskHandler.")

        effective_namespace = namespace if namespace is not None else self.xcom_namespace
        return self._xcom_manager.cleanup_namespace(effective_namespace, self.user_id, "manual_clear")

    def _configure_xcom_from_decorators(self) -> None:
        """Configure XCom settings from function decorators."""
        if not self.func:
            return

        # Check for xcom_task decorator (highest priority)
        config = getattr(self.func, '_xcom_task_config', None)
        if config is not None:
            self.enable_xcom = True
            self.xcom_push_key = config.get('push_key')
            self.xcom_pull_keys = config.get('pull_keys', {})
            self.xcom_ttl_seconds = config.get('ttl_seconds', 3600)
            # Use decorator namespace or keep task's namespace
            if config.get('namespace'):
                self.xcom_namespace = config['namespace']
            logger.debug(f"Configured XCom from @xcom_task decorator: {self.task_id}, namespace={self.xcom_namespace}")
            return

        # Check for individual decorators
        has_xcom_config = False

        config = getattr(self.func, '_xcom_push_config', None)
        if config is not None:
            self.enable_xcom = True
            self.xcom_push_key = config.get('key')
            self.xcom_ttl_seconds = config.get('ttl_seconds', 3600)
            if config.get('namespace'):
                self.xcom_namespace = config['namespace']
            has_xcom_config = True

        config = getattr(self.func, '_xcom_pull_config', None)
        if config is not None:
            self.enable_xcom = True
            self.xcom_pull_keys.update(config.get('keys', {}))
            if config.get('namespace'):
                self.xcom_namespace = config['namespace']
            has_xcom_config = True

        if has_xcom_config:
            logger.debug(f"Configured XCom from decorators: {self.task_id}, namespace={self.xcom_namespace}")

    # Optimized serialization methods
    def to_redis_dict(self) -> Dict[str, str]:
        """Redis storage with function metadata embedded in payload."""
        base_data = {
            "task_id": self.task_id,
            "user_id": self.user_id,
            "priority": str(self.priority.value),  # int as string
            "retry_count": str(self.retry_count),
            "max_retries": str(self.max_retries),
            "created_at": f"{self.created_at:.6f}",  # limited precision
            "execute_after": f"{self.execute_after:.6f}",
            "state": self.state.value,
            "depends_on": json.dumps(self.depends_on),
            "dependents": json.dumps(self.dependents),
            "auto_xcom": "1" if self.auto_xcom else "0",
        }

        # Add optional timestamps
        if self.started_at is not None:
            base_data["started_at"] = f"{self.started_at:.6f}"
        if self.finished_at is not None:
            base_data["finished_at"] = f"{self.finished_at:.6f}"

        # Embed function metadata in payload
        if self.func is not None:
            from fairque.core.function_registry import serialize_function
            func_meta = serialize_function(self.func)
            enhanced_payload = {
                **self.payload,
                "__function_meta__": func_meta,
                "__function_args__": list(self.args),  # Convert tuple to list for JSON
                "__function_kwargs__": self.kwargs,
                "__is_function_task__": True
            }
            base_data["payload"] = json.dumps(enhanced_payload, separators=(",", ":"))
        else:
            base_data["payload"] = json.dumps(self.payload, separators=(",", ":"))  # minimal JSON

        return base_data

    @classmethod
    def from_redis_dict(cls, data: Dict[str, str]) -> "Task":
        """Restore from Redis with function fallback resolution.

        Args:
            data: Dictionary from Redis with string keys and values

        Returns:
            Task instance restored from Redis data

        Raises:
            TaskSerializationError: If deserialization fails
        """
        payload = json.loads(data["payload"])

        # Create base task instance
        task = cls(
            task_id=data["task_id"],
            user_id=data["user_id"],
            priority=Priority(int(data["priority"])),
            payload={k: v for k, v in payload.items()
                    if not k.startswith("__function")},
            retry_count=int(data["retry_count"]),
            max_retries=int(data["max_retries"]),
            created_at=float(data["created_at"]),
            execute_after=float(data["execute_after"]),
            state=TaskState(data.get("state", TaskState.QUEUED.value)),
            depends_on=json.loads(data.get("depends_on", "[]")),
            dependents=json.loads(data.get("dependents", "[]")),
            auto_xcom=data.get("auto_xcom", "0") == "1",
            started_at=float(data["started_at"]) if data.get("started_at") else None,
            finished_at=float(data["finished_at"]) if data.get("finished_at") else None,
        )

        # Restore function if present
        if payload.get("__is_function_task__"):
            func_meta = payload.get("__function_meta__")
            if func_meta:
                from fairque.core.function_registry import try_deserialize_function
                func, strategy = try_deserialize_function(func_meta)
                if func is not None:
                    task.func = func
                    task.args = tuple(payload.get("__function_args__", []))
                    task.kwargs = payload.get("__function_kwargs__", {})
                    logger.debug(f"Function restored using {strategy}: {func_meta}")
                else:
                    logger.warning(f"Failed to restore function: {func_meta}")
                    # Function task without function - will be handled in TaskHandler

        return task

    def to_lua_args(self) -> List[str]:
        """Lua script argument list (optimized for array transmission).

        Returns:
            List of string arguments for Lua script
        """
        redis_dict = self.to_redis_dict()
        return [
            redis_dict["task_id"],
            redis_dict["user_id"],
            redis_dict["priority"],
            redis_dict["payload"],
            redis_dict["retry_count"],
            redis_dict["max_retries"],
            redis_dict["created_at"],
            redis_dict["execute_after"],
        ]

    @classmethod
    def from_lua_result(cls, lua_args: List[str]) -> "Task":
        """Restore from Lua script result (optimized array format).

        Args:
            lua_args: List of string arguments from Lua script

        Returns:
            Task instance restored from Lua result

        Raises:
            TaskSerializationError: If deserialization fails
        """
        if len(lua_args) != 8:
            raise TaskSerializationError(
                f"Invalid lua_args length: expected 8, got {len(lua_args)}"
            )

        return cls(
            task_id=lua_args[0],
            user_id=lua_args[1],
            priority=Priority(int(lua_args[2])),
            payload=json.loads(lua_args[3]),
            retry_count=int(lua_args[4]),
            max_retries=int(lua_args[5]),
            created_at=float(lua_args[6]),
            execute_after=float(lua_args[7]),
        )


@dataclass
class DLQEntry:
    """Simplified DLQ entry with failure type embedded."""

    entry_id: str           # UUID4 for DLQ entry
    original_task: Task     # Failed original task
    failure_type: str       # "failed"|"expired"|"poisoned"
    reason: str            # Failure reason description
    moved_at: float        # Timestamp when moved to DLQ
    retry_history: List[Dict[str, Any]] = field(default_factory=list)

    @classmethod
    def create(cls, task: Task, failure_type: str, reason: str) -> "DLQEntry":
        """Create new DLQ entry.

        Args:
            task: Original failed task
            failure_type: Type of failure ("failed", "expired", "poisoned")
            reason: Human-readable failure reason

        Returns:
            New DLQEntry instance
        """
        return cls(
            entry_id=str(uuid.uuid4()),
            original_task=task,
            failure_type=failure_type,
            reason=reason,
            moved_at=time.time(),
            retry_history=[],
        )

    def get_age_seconds(self) -> float:
        """Get age of this DLQ entry in seconds.

        Returns:
            Age in seconds since entry was moved to DLQ
        """
        return time.time() - self.moved_at
